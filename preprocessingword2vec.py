# -*- coding: utf-8 -*-
"""preprocessingWord2Vec

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yTWSibzucLChTEelYP0cUyrVUuBnewJi
"""

# preprocessing.py

import pandas as pd
import numpy as np
import re
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import os

def load_and_preprocess_data(train_path, val_path, test_path, w2v_path,
                             max_len_q=50, max_len_s=20, embedding_dim=128):

    print("üîπ Step 1: Loading datasets...")
    df_train = pd.read_excel(train_path)
    df_val = pd.read_excel(val_path)
    df_test = pd.read_excel(test_path)

    print(f"   ‚Ü™ df_train shape: {df_train.shape}")
    print(f"   ‚Ü™ df_val shape: {df_val.shape}")
    print(f"   ‚Ü™ df_test shape: {df_test.shape}")

    # Standardize column names
    df_train = df_train[['CHQ', 'Summary']].rename(columns={'CHQ': 'Question'})
    df_val = df_val[['NLM Question', 'Summary']].rename(columns={'NLM Question': 'Question'})
    df_test = df_test[['NLM Question', 'Summary']].rename(columns={'NLM Question': 'Question'})

    print("üîπ Step 2: Cleaning text and adding SOS/EOS tokens...")
    def clean_text(text):
        text = str(text).lower()
        text = re.sub(r'[^a-zA-Z0-9\s\?]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    for df in [df_train, df_val, df_test]:
        # Clean
        df['clean_question'] = df['Question'].apply(clean_text)
        df['clean_summary'] = df['Summary'].apply(lambda x: f"sos {clean_text(x)} eos")

        print("Sample clean_summary with SOS/EOS:", df['clean_summary'].iloc[0])

        # Tokenize
        df['question_tokens'] = df['clean_question'].apply(word_tokenize)
        df['summary_tokens'] = df['clean_summary'].apply(word_tokenize)

        print("Tokenized summary:", df['summary_tokens'].iloc[0])

    # Show sample cleaned + tokenized
    print("   ‚Ü™ Sample cleaned question:", df_train['clean_question'].iloc[0])
    print("   ‚Ü™ Sample tokenized question:", df_train['question_tokens'].iloc[0])
    print("   ‚Ü™ Sample cleaned summary (with sos/eos):", df_train['clean_summary'].iloc[0])
    print("   ‚Ü™ Sample tokenized summary:", df_train['summary_tokens'].iloc[0])

    print("üîπ Step 3: Load or Train Word2Vec embeddings...")

    all_tokens = df_train['question_tokens'].tolist() + df_train['summary_tokens'].tolist() + \
                    df_val['question_tokens'].tolist() + df_val['summary_tokens'].tolist() + \
                    df_test['question_tokens'].tolist() + df_test['summary_tokens'].tolist()

    # Train if not found
    if not os.path.isfile(w2v_path):
        print("   üîÑ Word2Vec model not found. Training and saving...")
        w2v_model = Word2Vec(sentences=all_tokens, vector_size=embedding_dim, window=5, min_count=1, workers=4)
        w2v_model.save(w2v_path)
    else:
        print("   ‚úÖ Loading frozen Word2Vec model...")
        w2v_model = Word2Vec.load(w2v_path)

    print(f"   ‚Ü™ Word2Vec vocabulary size: {len(w2v_model.wv)}")

    print("üîπ Step 4: Tokenizing and padding sequences...")
    all_text = [' '.join(tokens) for tokens in all_tokens]

    tokenizer = Tokenizer(num_words=50000, oov_token="<OOV>")
    tokenizer.fit_on_texts(all_text)
    vocab_size = len(tokenizer.word_index) + 1
    print(f"   ‚Ü™ Tokenizer vocab size: {vocab_size}")
    print(f"   ‚Ü™ Top 10 words: {list(tokenizer.word_index.items())[:10]}")

    def convert_and_pad(df, name="train"):
      q_seq = tokenizer.texts_to_sequences(df['question_tokens'])
      s_seq = tokenizer.texts_to_sequences(df['summary_tokens'])

      eos_id = tokenizer.word_index.get('eos')
      sos_id = tokenizer.word_index.get('sos')

      print(f"\nüîç Checking {name} set before padding...")

      # Debug stats before cleaning
      eos_inside, eos_at_end, sos_start = 0, 0, 0
      for i, seq in enumerate(s_seq):
          if seq and seq[0] == sos_id:
              sos_start += 1
          if eos_id in seq:
              eos_inside += 1
          if seq and seq[-1] == eos_id:
              eos_at_end += 1

      print(f"   üî∏ Total summaries: {len(s_seq)}")
      print(f"   üî∏ Start with SOS: {sos_start}")
      print(f"   üî∏ Contain EOS anywhere: {eos_inside}")
      print(f"   üî∏ EOS at END before fixing: {eos_at_end}")

      # Clean and append EOS at end
      for i in range(len(s_seq)):
          s_seq[i] = [tok for tok in s_seq[i] if tok != eos_id]  # remove eos
          s_seq[i] = s_seq[i][:max_len_s - 1]  # truncate if too long
          s_seq[i].append(eos_id)  # append eos

      # Pad both sequences
      q_pad = pad_sequences(q_seq, maxlen=max_len_q, padding='post')
      s_pad = pad_sequences(s_seq, maxlen=max_len_s, padding='post')

      # Post-debug sample
      print(f"   üî∏ Sample summary token IDs (first 2):")
      for s in s_pad[:2]:
          print("     ", s.tolist())
          if sos_id in s:
              sos_pos = s.tolist().index(sos_id)
          else:
              sos_pos = "‚ùå MISSING"
          if eos_id in s:
              eos_pos = len(s.tolist()) - 1 - s[::-1].tolist().index(eos_id)
          else:
              eos_pos = "‚ùå MISSING"
          print(f"     ‚Ü™ SOS position: {sos_pos}, EOS position: {eos_pos}")

      return q_pad, s_pad

    X_train, y_train = convert_and_pad(df_train, name="train")
    X_val, y_val = convert_and_pad(df_val, name="val")
    X_test, y_test = convert_and_pad(df_test, name="test")

    print("   ‚Ü™ X_train shape:", X_train.shape)
    print("   ‚Ü™ y_train shape:", y_train.shape)
    print("   ‚Ü™ Sample X_train[0]:", X_train[0])
    print("   ‚Ü™ Sample y_train[0]:", y_train[0])

    # --------------------------
    # üîç Data Quality Checks
    # --------------------------
    sos_token_id = tokenizer.word_index.get('sos')
    eos_token_id = tokenizer.word_index.get('eos')

    start_with_sos = sum(1 for seq in y_train if seq[0] == sos_token_id)
    end_with_eos = sum(1 for seq in y_train if eos_token_id in seq)
    eos_at_end = sum(1 for seq in y_train if seq[-1] == eos_token_id)
    non_zero_lengths = [sum(1 for tok in seq if tok != 0) for seq in y_train]
    average_len = sum(non_zero_lengths) / len(non_zero_lengths)

    print("\nüîç Quality Check on y_train:")
    print(f"   - Total summaries: {len(y_train)}")
    print(f"   - Start with SOS: {start_with_sos}")
    print(f"   - Contain EOS: {end_with_eos}")
    print(f"   - EOS at end of sequence: {eos_at_end}")
    print(f"   - Avg summary length: {average_len:.2f}")
    print(f"   - Max summary length allowed: {max_len_s}")

    print("üîπ Step 5: Building embedding matrix from Word2Vec...")
    embedding_matrix = np.zeros((vocab_size, embedding_dim))
    for word, index in tokenizer.word_index.items():
        if word in w2v_model.wv:
            embedding_matrix[index] = w2v_model.wv[word]
        else:
            embedding_matrix[index] = np.random.normal(scale=0.6, size=(embedding_dim,))
    print("   ‚Ü™ Embedding matrix shape:", embedding_matrix.shape)

    print("‚úÖ Preprocessing complete.\n")

    return {
        "X_train": X_train,
        "y_train": y_train,
        "X_val": X_val,
        "y_val": y_val,
        "X_test": X_test,
        "y_test": y_test,
        "tokenizer": tokenizer,
        "vocab_size": vocab_size,
        "max_len_q": max_len_q,
        "max_len_s": max_len_s,
        "embedding_matrix": embedding_matrix
    }